{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The solution combines a CNN and a FC Classifier and achieves pretty good performance (>94%)**. It :\n",
    "- crops the image around the characters and flips the cropped image (as we want the time dimension to correspond to the width of the image).  \n",
    "- uses a CNN to extract features from the cropped image.\n",
    "- reshapes the features to \"split\" them into 5 time-steps. \n",
    "- uses a FC Classifier to predict 5 characters : for each time-step, the output is the probability distribution of a character being at this step.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-10-15T15:37:25.048249Z",
     "iopub.status.busy": "2021-10-15T15:37:25.047907Z",
     "iopub.status.idle": "2021-10-15T15:37:25.053994Z",
     "shell.execute_reply": "2021-10-15T15:37:25.052556Z",
     "shell.execute_reply.started": "2021-10-15T15:37:25.048220Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "img_folder = './captcha/samples/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset provided by @fournierp https://www.kaggle.com/fournierp/captcha-version-2-images :\n",
    "- The dataset contains 1070 files. \n",
    "- Each file is an image representing a CAPTCHA image. \n",
    "- The image is either in png format (1040 png files) or in jpg format (30 jpg files). \n",
    "- The file name is composed of the 5 characters contained in the CAPTCHA image followed by the image format (e.g. 2g7nm.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-10-15T15:20:28.695889Z",
     "iopub.status.busy": "2021-10-15T15:20:28.695523Z",
     "iopub.status.idle": "2021-10-15T15:20:30.009930Z",
     "shell.execute_reply": "2021-10-15T15:20:30.008833Z",
     "shell.execute_reply.started": "2021-10-15T15:20:28.695859Z"
    }
   },
   "outputs": [],
   "source": [
    "img_2g7nm = mpimg.imread(img_folder + '2g7nm.png')\n",
    "img_34pcn = mpimg.imread(img_folder + '34pcn.png')\n",
    "img_bny23 = mpimg.imread(img_folder + 'bny23.png')\n",
    "img_c4mcm = mpimg.imread(img_folder + 'c4mcm.png')\n",
    "img_3c7de = mpimg.imread(img_folder + '3c7de.jpg')\n",
    "img_nxf2c = mpimg.imread(img_folder + 'nxf2c.jpg')\n",
    "img_pcmcc = mpimg.imread(img_folder + 'pcmcc.jpg')\n",
    "img_yge7c = mpimg.imread(img_folder + 'yge7c.jpg')\n",
    "samples = {'2g7nm.png':img_2g7nm, '34pcn.png':img_34pcn, 'bny23.png':img_bny23, 'c4mcm.png':img_c4mcm, \n",
    "           '3c7de.jpg':img_3c7de, 'nxf2c.jpg':img_nxf2c, 'pcmcc.jpg':img_pcmcc, 'yge7c.jpg':img_yge7c}\n",
    "\n",
    "fig=plt.figure(figsize=(20, 5))\n",
    "pos = 1\n",
    "for filename, img in samples.items():\n",
    "    fig.add_subplot(2, 4, pos)\n",
    "    pos = pos+1\n",
    "    plt.imshow(img)\n",
    "    plt.title('filename='+filename+' shape='+str(img.shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image has 50x200 pixels. That said, the shape of the array differs between jpg and png images indicating that jpg images are RGB and png are RGBA. Since images are black & white, R, G and B are all similar. Also for png images, there is no differecence in transparency (alpha is always 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, looking at the characters in the CAPTCHA images, one can notice that:\n",
    "- only 19 characters are used 2,3,4,5,6,7,8 and b,c,d,e,f,g,m,n,p,w,x,y\n",
    "- the frequency of each character is roughly the same with one exception : n is used twice often than other characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-10-15T15:20:32.042178Z",
     "iopub.status.busy": "2021-10-15T15:20:32.041773Z",
     "iopub.status.idle": "2021-10-15T15:20:34.593176Z",
     "shell.execute_reply": "2021-10-15T15:20:34.591812Z",
     "shell.execute_reply.started": "2021-10-15T15:20:32.042148Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['filename','extension','label','labelsize','char1','char2','char3','char4','char5'])\n",
    "i = 0\n",
    "for _, _, files in os.walk(img_folder):\n",
    "    for f in files:\n",
    "        df.loc[i,'filename'] = f\n",
    "        df.loc[i,'extension'] = f.split('.')[1]\n",
    "        df.loc[i,'label'] = f.split('.')[0]\n",
    "        df.loc[i,'labelsize'] = len(f.split('.')[0])\n",
    "        df.loc[i,'char1'] = f.split('.')[0][0]\n",
    "        df.loc[i,'char2'] = f.split('.')[0][1]\n",
    "        df.loc[i,'char3'] = f.split('.')[0][2]\n",
    "        df.loc[i,'char4'] = f.split('.')[0][3]\n",
    "        df.loc[i,'char5'] = f.split('.')[0][4]\n",
    "        i = i+1\n",
    "        \n",
    "#df.head()\n",
    "\n",
    "data = pd.DataFrame(df['char1'].value_counts()+df['char2'].value_counts()+df['char3'].value_counts()+df['char4'].value_counts()+df['char5'].value_counts()).reset_index()\n",
    "data.columns = ['character','count']\n",
    "\n",
    "sns.barplot(data=data, x='character', y='count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assess the performance of the different models on the validation dataset that contains 10% of the complete dataset. \n",
    "\n",
    "For each image in the validation set, we will compare the true label with the prediction. The true label and the prediction are (5,1) vectors. Therefore we will compare the vectors element-wise and count the number of good predictions element-wise. For each image, the score will be (nb of good predictions element-wise)/5. \n",
    "\n",
    "E.g. :   \n",
    "- the true label corresponding to the image 368y5.png is [1 4 6 18 3] (after transforming the characters to numerical values)\n",
    "- if the predicted label is [1 4 6 18 3], then the score for this image is 1\n",
    "- if the predicted label is [2 6 4 10 1], then the score for this image is 0\n",
    "- if the predicted label is [1 6 6 10 1], then the score for this image is 0.4 (2 characters correctly predicted)\n",
    "\n",
    "**The performance of the model will be the average of the scores for each image in the validation set. This number corresponds to the percentage of characters that are correctly predicted.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-15T15:20:43.673232Z",
     "iopub.status.busy": "2021-10-15T15:20:43.672772Z",
     "iopub.status.idle": "2021-10-15T15:20:43.680445Z",
     "shell.execute_reply": "2021-10-15T15:20:43.678876Z",
     "shell.execute_reply.started": "2021-10-15T15:20:43.673197Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_perf_metric(predictions, groundtruth):\n",
    "    if predictions.shape == groundtruth.shape:\n",
    "        return np.sum(predictions == groundtruth)/(predictions.shape[0]*predictions.shape[1])\n",
    "    else:\n",
    "        raise Exception('Error : the size of the arrays do not match. Cannot compute the performance metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T14:38:39.630788Z",
     "iopub.status.busy": "2021-07-31T14:38:39.63038Z",
     "iopub.status.idle": "2021-07-31T14:38:39.636782Z",
     "shell.execute_reply": "2021-07-31T14:38:39.635423Z",
     "shell.execute_reply.started": "2021-07-31T14:38:39.630751Z"
    }
   },
   "source": [
    "## Training and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section generates the training set that will be used to train the neural network and the validation set that will be used to evaluate the model performance.\n",
    "\n",
    "The training set will have 90% of the data :\n",
    "- X_train with 936 images. X_train shape will be (936,50,200,1) \n",
    "- y_train with 936 labels. y_train shape will be (936,5)  \n",
    "\n",
    "The validation set will have 10% of the data \n",
    "- X_val with 104 images. X_val shape will be (104,50,200,1) \n",
    "- y_val with 104 labels. y_val shape will be (104,5)\n",
    "\n",
    "The label corresponding to each image is a string corresponding to the filename minus the extension. As the neural net deals only with numerical values, we will have to map each character in the string to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-15T15:20:46.121926Z",
     "iopub.status.busy": "2021-10-15T15:20:46.121526Z",
     "iopub.status.idle": "2021-10-15T15:20:46.138825Z",
     "shell.execute_reply": "2021-10-15T15:20:46.137152Z",
     "shell.execute_reply.started": "2021-10-15T15:20:46.121897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionaries that will be used to convert characters to integers and vice-versa\n",
    "vocabulary = {'2','3','4','5','6','7','8','b','c','d','e','f','g','m','n','p','w','x','y'}\n",
    "char_to_num = {'2':0,'3':1,'4':2,'5':3,'6':4,'7':5,'8':6,'b':7,'c':8,'d':9,'e':10,'f':11,'g':12,'m':13,'n':14,'p':15,'w':16,'x':17,'y':18}\n",
    "\n",
    "##############################################################################################################################\n",
    "# This function encodes a single sample. \n",
    "# Inputs :\n",
    "# - img_path : the string representing the image path e.g. '/kaggle/input/captcha-version-2-images/samples/samples/6n6gg.jpg'\n",
    "# - label : the string representing the label e.g. '6n6gg'\n",
    "# - crop : boolean, if True the image is cropped around the characters and resized to the original size.\n",
    "# Outputs :\n",
    "# - a multi-dimensional array reprensenting the image. Its shape is (50, 200, 1)\n",
    "# - an array of integers representing the label after encoding the characters to integer. E.g [6,16,6,14,14] for '6n6gg' \n",
    "##############################################################################################################################\n",
    "def encode_single_sample(img_path, label, crop):\n",
    "    # Read image file and returns a tensor with dtype=string\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # Decode and convert to grayscale (this conversion does not cause any information lost and reduces the size of the tensor)\n",
    "    # This decode function returns a tensor with dtype=uint8\n",
    "    img = tf.io.decode_png(img, channels=1) \n",
    "    # Scales and returns a tensor with dtype=float32\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # Crop and resize to the original size : \n",
    "    # top-left corner = offset_height, offset_width in image = 0, 25 \n",
    "    # lower-right corner is at offset_height + target_height, offset_width + target_width = 50, 150\n",
    "    if(crop==True):\n",
    "        img = tf.image.crop_to_bounding_box(img, offset_height=0, offset_width=25, target_height=50, target_width=125)\n",
    "        img = tf.image.resize(img,size=[50,200],method='bilinear', preserve_aspect_ratio=False,antialias=False, name=None)\n",
    "    # Transpose the image because we want the time dimension to correspond to the width of the image.\n",
    "    img = tf.transpose(img, perm=[1, 0, 2])\n",
    "    # Converts the string label into an array with 5 integers. E.g. '6n6gg' is converted into [6,16,6,14,14]\n",
    "    label = list(map(lambda x:char_to_num[x], label))\n",
    "    return img.numpy(), label\n",
    "\n",
    "def create_train_and_validation_datasets(crop=False):\n",
    "    # Loop on all the files to create X whose shape is (1040, 50, 200, 1) and y whose shape is (1040, 5)\n",
    "    X, y = [],[]\n",
    "\n",
    "    for _, _, files in os.walk(img_folder):\n",
    "        for f in files:\n",
    "            # To start, let's ignore the jpg images\n",
    "            label = f.split('.')[0]\n",
    "            extension = f.split('.')[1]\n",
    "            if extension=='png':\n",
    "                img, label = encode_single_sample(img_folder+f, label,crop)\n",
    "                X.append(img)\n",
    "                y.append(label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split X, y to get X_train, y_train, X_val, y_val \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X.reshape(1040, 10000), y, test_size=0.1, shuffle=True, random_state=42)\n",
    "    X_train, X_val = X_train.reshape(936,200,50,1), X_val.reshape(104,200,50,1)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a few examples :   \n",
    "- images on the 1st row are transposed, cropped around the characters and resized as expected by the \"classifier-like\" model\n",
    "- images on the 2nd row are transposed only as expected by the CNN LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-10-15T15:20:49.411129Z",
     "iopub.status.busy": "2021-10-15T15:20:49.410706Z",
     "iopub.status.idle": "2021-10-15T15:21:02.279335Z",
     "shell.execute_reply": "2021-10-15T15:21:02.278178Z",
     "shell.execute_reply.started": "2021-10-15T15:20:49.411096Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = create_train_and_validation_datasets(crop=True)\n",
    "X_train_, X_val_, y_train_, y_val_ = create_train_and_validation_datasets(crop=False)\n",
    "\n",
    "fig=plt.figure(figsize=(20, 10))\n",
    "fig.add_subplot(2, 4, 1)\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "#plt.imshow(X_train[0].transpose((1,0,2)), cmap='gray')\n",
    "plt.title('Image from X_train with label '+ str(y_train[0]))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(2, 4, 2)\n",
    "plt.imshow(X_train[935], cmap='gray')\n",
    "#plt.imshow(X_train[935].transpose((1,0,2)), cmap='gray')\n",
    "plt.title('Image from X_train with label '+ str(y_train[935]))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(2, 4, 3)\n",
    "plt.imshow(X_val[0], cmap='gray')\n",
    "#plt.imshow(X_val[0].transpose((1,0,2)), cmap='gray')\n",
    "plt.title('Image from X_val with label '+ str(y_val[0]))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(2, 4, 4)\n",
    "plt.imshow(X_val[103], cmap='gray')\n",
    "#plt.imshow(X_val[103].transpose((1,0,2)), cmap='gray')\n",
    "plt.title('Image from X_val with label '+ str(y_val[103]))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(2, 4, 5)\n",
    "plt.imshow(X_train_[0], cmap='gray')\n",
    "plt.title('Image from X_train with label '+ str(y_train_[0]))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(2, 4, 6)\n",
    "plt.imshow(X_train_[935], cmap='gray')\n",
    "plt.title('Image from X_train with label '+ str(y_train_[935]))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(2, 4, 7)\n",
    "plt.imshow(X_val_[0], cmap='gray')\n",
    "plt.title('Image from X_val with label '+ str(y_val_[0]))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(2, 4, 8)\n",
    "plt.imshow(X_val_[103], cmap='gray')\n",
    "plt.title('Image from X_val with label '+ str(y_val_[103]))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-15T15:25:09.315404Z",
     "iopub.status.busy": "2021-10-15T15:25:09.315002Z",
     "iopub.status.idle": "2021-10-15T15:25:09.509229Z",
     "shell.execute_reply": "2021-10-15T15:25:09.508072Z",
     "shell.execute_reply.started": "2021-10-15T15:25:09.315374Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    # Inputs to the model\n",
    "    input_img = layers.Input(shape=(200,50,1), name=\"image\", dtype=\"float32\") \n",
    "\n",
    "    # First conv block\n",
    "    x = layers.Conv2D(32,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv1\")(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    # Second conv block\n",
    "    x = layers.Conv2D(64,(3, 3),activation=\"relu\",kernel_initializer=\"he_normal\",padding=\"same\",name=\"Conv2\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "\n",
    "    # We have used two max pool with pool size and strides 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
    "    # filters in the last layer is 64 --> output volume shape = (50,12,64) \n",
    "    # Reshape to \"split\" the volume in 5 time-steps\n",
    "    x = layers.Reshape(target_shape=(5, 7680), name=\"reshape\")(x)\n",
    "\n",
    "    # FC layers\n",
    "    x = layers.Dense(256, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x)\n",
    "   \n",
    "    # Output layer\n",
    "    output = layers.Dense(19, activation=\"softmax\", name=\"dense3\")(x) \n",
    "    \n",
    "    # Define the model\n",
    "    model = keras.models.Model(inputs=input_img, outputs=output, name=\"ocr_classifier_based_model\")\n",
    "    \n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-31T15:43:06.876315Z",
     "iopub.status.busy": "2021-07-31T15:43:06.875921Z",
     "iopub.status.idle": "2021-07-31T15:43:06.880712Z",
     "shell.execute_reply": "2021-07-31T15:43:06.879394Z",
     "shell.execute_reply.started": "2021-07-31T15:43:06.876256Z"
    }
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-10-15T15:25:12.399963Z",
     "iopub.status.busy": "2021-10-15T15:25:12.399515Z",
     "iopub.status.idle": "2021-10-15T15:25:43.409042Z",
     "shell.execute_reply": "2021-10-15T15:25:43.408093Z",
     "shell.execute_reply.started": "2021-10-15T15:25:12.399916Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = create_train_and_validation_datasets(crop=True)\n",
    "history = model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-10-15T15:25:53.839218Z",
     "iopub.status.busy": "2021-10-15T15:25:53.838264Z",
     "iopub.status.idle": "2021-10-15T15:25:54.195187Z",
     "shell.execute_reply": "2021-10-15T15:25:54.193506Z",
     "shell.execute_reply.started": "2021-10-15T15:25:53.839171Z"
    }
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20, 5))\n",
    "# summarize history for accuracy\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this model to predict the labels for the 104 images of the validation set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-15T15:26:05.902526Z",
     "iopub.status.busy": "2021-10-15T15:26:05.902183Z",
     "iopub.status.idle": "2021-10-15T15:26:07.038523Z",
     "shell.execute_reply": "2021-10-15T15:26:07.037457Z",
     "shell.execute_reply.started": "2021-10-15T15:26:05.902497Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val) # y_pred shape = (104,50,19)\n",
    "y_pred = np.argmax(y_pred, axis=2)\n",
    "num_to_char = {'-1':'UKN','0':'2','1':'3','2':'4','3':'5','4':'6','5':'7','6':'8','7':'b','8':'c','9':'d','10':'e','11':'f','12':'g','13':'m','14':'n','15':'p','16':'w','17':'x','18':'y'}\n",
    "nrow = 1\n",
    "fig=plt.figure(figsize=(20, 5))\n",
    "for i in range(0,10):\n",
    "    if i>4: nrow = 2\n",
    "    fig.add_subplot(nrow, 5, i+1)\n",
    "    plt.imshow(X_val[i].transpose((1,0,2)),cmap='gray')\n",
    "    plt.title('Prediction : ' + str(list(map(lambda x:num_to_char[str(x)], y_pred[i]))))\n",
    "    plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the function compute_perf_metric to calculate the performance metric of this model. Reminder : this metric corresponds to the percentage of characters that are correctly predicted (and this % is calculated using the 104 images hence 520 characters of the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-15T15:26:20.104529Z",
     "iopub.status.busy": "2021-10-15T15:26:20.104042Z",
     "iopub.status.idle": "2021-10-15T15:26:20.114469Z",
     "shell.execute_reply": "2021-10-15T15:26:20.112663Z",
     "shell.execute_reply.started": "2021-10-15T15:26:20.104499Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_perf_metric(y_pred, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
